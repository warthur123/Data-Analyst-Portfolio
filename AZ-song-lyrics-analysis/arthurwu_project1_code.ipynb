{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import time\n",
    "import random\n",
    "import re\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Initiate all lists to store scraped data\n",
    "song_url_list = [] \n",
    "song_artist_list = []\n",
    "song_title_list = []\n",
    "song_year_list = []\n",
    "song_lyrics_list = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get response for \"w\" artists page\n",
    "letter_url = \"https://www.azlyrics.com/w.html\"\n",
    "letter_page = requests.get(letter_url)\n",
    "letter_soup = BeautifulSoup(letter_page.content, 'lxml')\n",
    "print(\"status:\", letter_page.status_code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#scrape letter \"w\" page for artists\n",
    "artist_columns = letter_soup.find_all(\"div\", class_=\"col-sm-6 text-center artist-col\")\n",
    "left = artist_columns[0]\n",
    "right = artist_columns[1]\n",
    "    \n",
    "left_artists = left.find_all(\"a\")\n",
    "right_artists = right.find_all(\"a\")\n",
    "artist_list = left_artists + right_artists\n",
    "\n",
    "print(len(artist_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#initialize count to iterate\n",
    "count = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Scrape to get populate lists with songs in \"W\"\n",
    "while count < len(artist_list):\n",
    "    artist_link = artist_list[count].get(\"href\")\n",
    "    #artist_link = artist_list[0].get(\"href\")\n",
    "    artist_url = \"https://www.azlyrics.com/\" + artist_link\n",
    "    artist_page = requests.get(artist_url)\n",
    "    \n",
    "    print(count, artist_url)\n",
    "    print(\"status:\", artist_page.status_code)\n",
    "    \n",
    "    artist_soup = BeautifulSoup(artist_page.content, 'lxml')\n",
    "\n",
    "    # artist page\n",
    "    artist_name = artist_soup.find(\"h1\").findChild(\"strong\").text\n",
    "    all_songs = artist_soup.find_all(\"div\", class_=\"listalbum-item\")\n",
    "\n",
    "    for song in all_songs:\n",
    "        #song_artist\n",
    "        #print(artist_name[:-7])\n",
    "        song_artist_list.append(artist_name[:-7])\n",
    "    \n",
    "    \n",
    "        #song_title\n",
    "        song_title = song.find('a').text\n",
    "        #print(song_title)\n",
    "        song_title_list.append(song_title)\n",
    "        \n",
    "        \n",
    "        #song_year\n",
    "        song_year = song.find_previous_sibling(\"div\", class_=\"album\")\n",
    "        if song_year:\n",
    "            year = song_year.text\n",
    "            if re.search(r\"\\(\\d+\\)\", year):\n",
    "                match = re.search(r\"\\((\\d+)\\)\", year)\n",
    "                #print(match.group(1))\n",
    "                song_year_list.append(match.group(1))\n",
    "            else: \n",
    "                #print(\"\")\n",
    "                song_year_list.append(\"\")\n",
    "        else:\n",
    "            #print(\"\")\n",
    "            song_year_list.append(\"\")\n",
    "    \n",
    "    \n",
    "        #song_url\n",
    "        song_link = song.find('a').get('href')\n",
    "        if \"https://\" in song_link:\n",
    "            #print(song_link)\n",
    "            song_url_list.append(song_link)\n",
    "        else:\n",
    "            song_link_edit = \"https://www.azlyrics.com/\" + song_link[3:]\n",
    "            #print(song_link_edit)\n",
    "            song_url_list.append(song_link_edit)\n",
    "    \n",
    "    \n",
    "    print(len(song_artist_list), \"|\", len(song_title_list), \"|\", len(song_year_list), \"|\", len(song_url_list))\n",
    "    \n",
    "    count += 1\n",
    "    time.sleep(random.randint(7,28))\n",
    "        \n",
    "   \n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create new dataframe with values from lists of data\n",
    "data = {'artist_name': song_artist_list, 'song_title': song_title_list, 'year': song_year_list, 'lyrics_url': song_url_list}\n",
    "df = pd.DataFrame(data, columns= ['artist_name', 'song_title','year','lyrics_url'])\n",
    "\n",
    "df['year'] = df['year'].apply(pd.to_numeric)\n",
    "\n",
    "df.info()\n",
    "df.tail()\n",
    "\n",
    "#save all data to csv\n",
    "df.to_csv('arthurwu_project1_allSongs_urls.csv', index = False, header=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#new dataframe for sampled songs\n",
    "sample_df = pd.DataFrame()\n",
    "\n",
    "# get up to 10 samples from every year\n",
    "for i in range(int(df[\"year\"].min()), int(df[\"year\"].max() + 1)):\n",
    "    year_songs = df[df[\"year\"] == i]\n",
    "    if len(year_songs) < 10 and len(year_songs) > 0:\n",
    "        sample_df = pd.concat([sample_df, year_songs], axis=0, ignore_index=True)\n",
    "    elif len(year_songs) > 10:   \n",
    "        year_sample = year_songs.sample(10)\n",
    "        sample_df = pd.concat([sample_df, year_sample], axis=0, ignore_index=True)\n",
    "    else:\n",
    "        print(\"no songs\", i)\n",
    "    \n",
    "sample_df.info()\n",
    "sample_df.tail(15)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#initialize count to iterate\n",
    "url_count = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# get lyrics using sample url list\n",
    "while url_count < len(sample_df[\"lyrics_url\"]):\n",
    "    \n",
    "    #url request\n",
    "    song_page = requests.get(sample_df[\"lyrics_url\"][url_count])\n",
    "    #song_page = requests.get(song_url)\n",
    "    song_soup = BeautifulSoup(song_page.content, 'lxml')\n",
    "    print(url_count, sample_df[\"lyrics_url\"][url_count])\n",
    "    \n",
    "    # find and save lyrics data\n",
    "    song_lyrics = song_soup.find_all(\"div\", class_=None)[1].text.replace(\",\", \";\")\n",
    "    song_lyrics_list.append(song_lyrics)\n",
    "    \n",
    "    \n",
    "    url_count += 1\n",
    "    time.sleep(random.randint(7,28))\n",
    "\n",
    "print(\"done\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#add lyrics array to sample dataframe\n",
    "sample_df[\"lyrics_text\"] = song_lyrics_list    # sample_df and song_lyrics_list must match in len\n",
    "sample_df.info()\n",
    "sample_df.head()\n",
    "\n",
    "#save sample dataframe as csv\n",
    "sample_df1.to_csv('arthurwu_project1_sampledSongs_urls.csv', index = False, header=True)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
